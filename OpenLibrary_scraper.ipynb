{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQZWHHKzusn0+s1jaTasAO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matoussi-roua/openlibrary-spider-scraper/blob/main/OpenLibrary_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Install Scrapy and Playwright**\n",
        "\n",
        "\n",
        "*   scrapy → Python framework for web scraping.\n",
        "\n",
        "*   scrapy-playwright → Enables Scrapy to handle dynamic websites (those that load content with JavaScript) by controlling a real browser (Chromium, Firefox, or WebKit).\n",
        "\n",
        "*   !playwright install → Downloads browser binaries needed by Playwright to run and render pages.\n",
        "\n"
      ],
      "metadata": {
        "id": "rliZBT8Ipk-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapy scrapy-playwright\n",
        "!playwright install\n",
        "\n",
        "# Install asyncio reactor\n",
        "from twisted.internet import asyncioreactor\n",
        "asyncioreactor.install()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOm7bZHWaLDr",
        "outputId": "b04c3045-e1ae-4274-f9e6-cb4587b14dc8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.12/dist-packages (2.13.3)\n",
            "Requirement already satisfied: scrapy-playwright in /usr/local/lib/python3.12/dist-packages (0.0.44)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.12/dist-packages (from scrapy) (43.0.3)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from scrapy) (1.3.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from scrapy) (0.7.1)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from scrapy) (0.12.2)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from scrapy) (1.3.2)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from scrapy) (5.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from scrapy) (25.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from scrapy) (1.10.0)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.12/dist-packages (from scrapy) (0.5.0)\n",
            "Requirement already satisfied: pydispatcher>=2.0.5 in /usr/local/lib/python3.12/dist-packages (from scrapy) (2.0.7)\n",
            "Requirement already satisfied: pyopenssl>=22.0.0 in /usr/local/lib/python3.12/dist-packages (from scrapy) (24.2.1)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from scrapy) (1.8.0)\n",
            "Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.12/dist-packages (from scrapy) (24.2.0)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.12/dist-packages (from scrapy) (5.3.0)\n",
            "Requirement already satisfied: twisted>=21.7.0 in /usr/local/lib/python3.12/dist-packages (from scrapy) (25.5.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from scrapy) (2.3.1)\n",
            "Requirement already satisfied: zope-interface>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from scrapy) (8.0)\n",
            "Requirement already satisfied: playwright>=1.15 in /usr/local/lib/python3.12/dist-packages (from scrapy-playwright) (1.55.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=37.0.0->scrapy) (2.0.0)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.12/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
            "Requirement already satisfied: pyee<14,>=13 in /usr/local/lib/python3.12/dist-packages (from playwright>=1.15->scrapy-playwright) (13.0.0)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright>=1.15->scrapy-playwright) (3.2.4)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from service-identity>=18.1.0->scrapy) (25.3.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.12/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.12/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.2)\n",
            "Requirement already satisfied: automat>=24.8.0 in /usr/local/lib/python3.12/dist-packages (from twisted>=21.7.0->scrapy) (25.4.16)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.12/dist-packages (from twisted>=21.7.0->scrapy) (23.10.4)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.12/dist-packages (from twisted>=21.7.0->scrapy) (21.0.0)\n",
            "Requirement already satisfied: incremental>=24.7.0 in /usr/local/lib/python3.12/dist-packages (from twisted>=21.7.0->scrapy) (24.7.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from twisted>=21.7.0->scrapy) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from zope-interface>=5.1.0->scrapy) (75.2.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from tldextract->scrapy) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from tldextract->scrapy) (2.32.4)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.12/dist-packages (from tldextract->scrapy) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract->scrapy) (3.19.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2025.8.3)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:269:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:934:14)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:1056:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:1045:7)\n",
            "    at async i.<anonymous> (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/cli/program.js:217:7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Scrapy and CrawlerProcess**\n",
        "scrapy provides the spider logic, and CrawlerProcess lets you execute the spider and collect scraped data programmatically."
      ],
      "metadata": {
        "id": "N2QWvD7eqJtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n"
      ],
      "metadata": {
        "id": "gepDzrHrWQ4N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup for Asynchronous Scrapy Execution**\n",
        "\n",
        "asyncio: Runs multiple tasks concurrently without blocking.\n",
        "\n",
        "reactor: Twisted’s event loop that manages async network operations for Scrapy.\n",
        "\n",
        "asyncioreactor: Integrates Twisted with Python’s asyncio, ensuring Scrapy works in async environments like Jupyter/Colab."
      ],
      "metadata": {
        "id": "yk5c4eN2qd2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from twisted.internet import reactor\n",
        "from twisted.internet import asyncioreactor\n",
        "\n",
        "# Explicitly install the asyncioreactor before running\n",
        "# Removed explicit install as it's often already installed in Colab\n",
        "# try:\n",
        "#     asyncioreactor.install()\n",
        "# except Exception as e:\n",
        "#     print(f\"Could not install asyncioreactor: {e}\")\n"
      ],
      "metadata": {
        "id": "0KrlHnFaqdGf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OpenLibrarySpider**\n",
        "\n",
        "This is a Scrapy spider that scrapes book information from Open Library.\n",
        "\n",
        "Starts at https://openlibrary.org/ and finds all books on the page.\n",
        "\n",
        "Follows each book link to get details: title, author, rating, publisher, pages, description, subjects.\n",
        "\n",
        "yield sends data to JSON or CSV output."
      ],
      "metadata": {
        "id": "okx-DSgLrGbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class OpenLibrarySpider(scrapy.Spider):\n",
        "    name = \"openlibrary\"\n",
        "    start_urls = [\n",
        "        'https://openlibrary.org/'  # replace with your starting page\n",
        "    ]\n",
        "\n",
        "    def parse(self, response):\n",
        "        books = response.css('div.book.carousel__item')\n",
        "        for book in books:\n",
        "            relative_url = book.css('div.book-cover a::attr(href)').get()\n",
        "            if relative_url:\n",
        "                full_url = response.urljoin(relative_url)\n",
        "                print(f\"[STEP] Following book link: {full_url}\")  # Debug output\n",
        "                yield response.follow(full_url, callback=self.parse_book_details)\n",
        "\n",
        "    def parse_book_details(self, response):\n",
        "        title = response.css('h1.work-title::text').get()\n",
        "        print(f\"[STEP] Parsing book details: {title}\")  # Debug output\n",
        "        subtitle = response.css('h2.work-subtitle::text').get()\n",
        "        author = response.css('h2.edition-byline a::text').get()\n",
        "        rating = response.css('ul.readers-stats meta[itemprop=\"ratingValue\"]::attr(content)').get()\n",
        "        rating_count = response.css('ul.readers-stats meta[itemprop=\"ratingCount\"]::attr(content)').get()\n",
        "        pub_date = response.css('div.edition-omniline-item span[itemprop=\"datePublished\"]::text').get()\n",
        "        publisher = response.css('div.edition-omniline-item a[itemprop=\"publisher\"]::text').get()\n",
        "        language = response.css('div.edition-omniline-item span[itemprop=\"inLanguage\"] a::text').get()\n",
        "        pages = response.css('div.edition-omniline-item span[itemprop=\"numberOfPages\"]::text').get()\n",
        "        description = response.css('div.book-description .read-more__content > p::text').get(default='').strip()\n",
        "        subjects = response.css('div.subjects-content a::text').getall()\n",
        "        print(f\"[STEP] Yielding data for: {title}\")  # Debug output\n",
        "        yield {\n",
        "            'title': title,\n",
        "            'subtitle': subtitle,\n",
        "            'author': author,\n",
        "            'rating': rating,\n",
        "            'rating_count': rating_count,\n",
        "            'publication_date': pub_date,\n",
        "            'publisher': publisher,\n",
        "            'language': language,\n",
        "            'pages': pages,\n",
        "            'description': description,\n",
        "            'subjects': subjects\n",
        "            }\n",
        "\n"
      ],
      "metadata": {
        "id": "xoPHTM0eVHfD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run Spider Programmatically**\n",
        "run_spider(): Asynchronously starts the Scrapy crawler.\n",
        "\n",
        "CrawlerProcess(settings=...): Configures output formats (JSON & CSV) and logging.\n",
        "\n",
        "process.crawl(OpenLibrarySpider): Schedules the spider to run.\n",
        "\n",
        "await process.start(stop_after_crawl=True): Runs the spider and waits until it finishes.\n",
        "\n",
        "**Asyncio loop handling:**\n",
        "\n",
        "If a loop is already running (common in Colab/Jupyter), it adds the spider as a task.\n",
        "\n",
        "Otherwise, it runs the spider in a new loop using asyncio.run()."
      ],
      "metadata": {
        "id": "oV5lpMqbrY8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Run spider programmatically\n",
        "# -----------------------------\n",
        "\n",
        "async def run_spider():\n",
        "    process = CrawlerProcess(settings={\n",
        "        \"FEEDS\": {\n",
        "            \"books.json\": {\"format\": \"json\", \"encoding\": \"utf-8\", \"indent\": 4},\n",
        "            \"books.csv\": {\"format\": \"csv\", \"encoding\": \"utf-8\"},\n",
        "        },\n",
        "        \"LOG_LEVEL\": \"INFO\",\n",
        "        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\", # Ensure this is also in process settings\n",
        "    })\n",
        "\n",
        "    # Schedule the spider to be run\n",
        "    process.crawl(OpenLibrarySpider)\n",
        "\n",
        "    # Start the crawler within the asyncio loop\n",
        "    await process.start(stop_after_crawl=True)\n",
        "\n",
        "\n",
        "# Get the current running loop and create a task to run the spider\n",
        "try:\n",
        "    loop = asyncio.get_running_loop()\n",
        "    loop.create_task(run_spider())\n",
        "except RuntimeError:\n",
        "    # If no loop is running (less common in Colab), use asyncio.run()\n",
        "    asyncio.run(run_spider())"
      ],
      "metadata": {
        "id": "jwDqnRCueGLR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/books.csv\")\n",
        "files.download(\"/content/books.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "HFKDjP2DYOsD",
        "outputId": "76d08c69-a3e8-4569-b5eb-327ac60387f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_99642e50-c9d4-4a91-a771-36af23e7639f\", \"books.csv\", 27323)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bbeff979-70e5-473d-820e-9b67fbe9a9a1\", \"books.json\", 43982)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/books.csv\")\n",
        "df.to_excel(\"books.xlsx\", index=False)\n",
        "print(\"Excel file generated: books.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd5SFyYllG5A",
        "outputId": "5d990640-b593-45a5-c34d-b8bb017a385e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excel file generated: books.xlsx\n"
          ]
        }
      ]
    }
  ]
}